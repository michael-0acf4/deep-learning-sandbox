{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x_exp = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return x_exp / np.sum(x_exp, axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy has the same broadcasting rules as PyTorch when there is no ambiguity \n",
    "B, C, E = 11, 123, 30000 # batch, context length, embedding dim\n",
    "q = np.random.normal(loc=0, scale=1, size=(B, C, E))\n",
    "k = np.random.normal(loc=0, scale=1, size=(C, E))\n",
    "v = np.random.normal(loc=0, scale=1, size=(C, E))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "variance [[1.06 1.   1.13 ... 1.01 0.99 1.05]\n",
      " [0.96 1.01 1.04 ... 0.85 0.94 1.1 ]\n",
      " [0.98 0.87 0.98 ... 0.9  1.12 1.01]\n",
      " ...\n",
      " [1.01 1.08 0.9  ... 0.84 0.95 1.15]\n",
      " [0.99 1.01 0.84 ... 0.9  0.99 0.87]\n",
      " [1.12 1.   1.12 ... 0.89 1.17 1.06]]\n"
     ]
    }
   ],
   "source": [
    "# Since q and k are independently random vectors i.e. N(μ=0, σ^2=1), the dot product grows proportionally to E..\n",
    "logits = q @ k.T\n",
    "\n",
    "# softmax would converge to one hot vectors if uncontrolled\n",
    "# this is one way to regularize the values (sqrt(E) because we are summing E independent terms)\n",
    "logits_regd = logits / np.sqrt(E)\n",
    "print(\"variance\", np.var(logits_regd, axis=-1).round(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11, 123, 123) (11, 123, 123) [[1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " ...\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]\n",
      " [1. 1. 1. ... 1. 1. 1.]]\n",
      "(11, 123, 30000)\n"
     ]
    }
   ],
   "source": [
    "# Classical self-attention\n",
    "weights = softmax(logits)\n",
    "print(logits.shape, weights.shape, weights.sum(-1))\n",
    "attention = weights @ v\n",
    "print(attention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 0. mask\n",
      " [[  0. -inf -inf ... -inf -inf -inf]\n",
      " [  0.   0. -inf ... -inf -inf -inf]\n",
      " [  0.   0.   0. ... -inf -inf -inf]\n",
      " ...\n",
      " [  0.   0.   0. ...   0. -inf -inf]\n",
      " [  0.   0.   0. ...   0.   0. -inf]\n",
      " [  0.   0.   0. ...   0.   0.   0.]]\n",
      "idx 0. logit\n",
      " [[ 0.5   0.82 -0.27 ...  0.8  -0.73 -0.12]\n",
      " [ 0.92  0.06  0.73 ... -0.24  0.03 -0.32]\n",
      " [ 0.54  0.66  0.68 ... -0.44 -1.71 -1.65]\n",
      " ...\n",
      " [-1.4  -0.95  0.07 ... -0.41 -0.42 -0.93]\n",
      " [ 2.1  -0.56  0.62 ... -1.16 -0.78 -0.3 ]\n",
      " [ 0.59  1.14  1.33 ... -1.5  -0.43  0.35]]\n",
      "idx 0. masked\n",
      " [[ 0.5   -inf  -inf ...  -inf  -inf  -inf]\n",
      " [ 0.92  0.06  -inf ...  -inf  -inf  -inf]\n",
      " [ 0.54  0.66  0.68 ...  -inf  -inf  -inf]\n",
      " ...\n",
      " [-1.4  -0.95  0.07 ... -0.41  -inf  -inf]\n",
      " [ 2.1  -0.56  0.62 ... -1.16 -0.78  -inf]\n",
      " [ 0.59  1.14  1.33 ... -1.5  -0.43  0.35]]\n",
      "idx 0. softmax\n",
      " [[1.         0.         0.         ... 0.         0.         0.        ]\n",
      " [0.70266065 0.29733935 0.         ... 0.         0.         0.        ]\n",
      " [0.30508541 0.34398284 0.35093175 ... 0.         0.         0.        ]\n",
      " ...\n",
      " [0.00115379 0.00180951 0.00501812 ... 0.00310513 0.         0.        ]\n",
      " [0.03959958 0.00276992 0.00901436 ... 0.00152016 0.00222291 0.        ]\n",
      " [0.00939482 0.0162836  0.01969094 ... 0.00116202 0.00338772 0.00739023]]\n",
      "idx 0. probs check\n",
      " [1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1. 1.\n",
      " 1. 1. 1.]\n",
      "Full output shape (11, 123, 30000)\n"
     ]
    }
   ],
   "source": [
    "# Causal self-attention\n",
    "mask = np.where(np.triu(np.ones((C, C)), k=1) == 1, -np.inf, 0) # Fill upper triangle with -inf\n",
    "print(\"idx 0. mask\\n\", mask)\n",
    "print(\"idx 0. logit\\n\", logits_regd[0].round(2))\n",
    "print(\"idx 0. masked\\n\", logits_regd[0].round(2) + mask)\n",
    "print(\"idx 0. softmax\\n\", softmax(logits_regd[0].round(2) + mask))\n",
    "print(\"idx 0. probs check\\n\", softmax(logits_regd[0].round(2) + mask).sum(-1))\n",
    "\n",
    "causal_self_attention = softmax(logits_regd + mask) @ v\n",
    "print(\"Full output shape\", causal_self_attention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
