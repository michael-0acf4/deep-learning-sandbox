{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x_exp = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return x_exp / np.sum(x_exp, axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy has the same broadcasting rules as PyTorch when there is no ambiguity \n",
    "B, C, E = 2, 5, 3 # batch, context length, embedding dim\n",
    "q = np.random.rand(B, C, E)\n",
    "k = np.random.rand(C, E)\n",
    "v = np.random.rand(C, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Since q and k are independently random vectors i.e. N(μ=0, σ^2=1), the dot product grows proportionally to E..\n",
    "logits = q @ k.T\n",
    "\n",
    "# softmax would converge to one hot vectors if uncontrolled\n",
    "# this is one way to regularize the values (sqrt(E) because we are summing E independent terms)\n",
    "logits_regd = logits / np.sqrt(E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5, 5) (2, 5, 5) [[1. 1. 1. 1. 1.]\n",
      " [1. 1. 1. 1. 1.]]\n",
      "(2, 5, 3)\n"
     ]
    }
   ],
   "source": [
    "# Classical self-attention\n",
    "weights = softmax(logits)\n",
    "print(logits.shape, weights.shape, weights.sum(-1))\n",
    "attention = weights @ v\n",
    "print(attention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "idx 0. mask\n",
      " [[  0. -inf -inf -inf -inf]\n",
      " [  0.   0. -inf -inf -inf]\n",
      " [  0.   0.   0. -inf -inf]\n",
      " [  0.   0.   0.   0. -inf]\n",
      " [  0.   0.   0.   0.   0.]]\n",
      "idx 0. logit\n",
      " [[0.7  0.38 0.2  0.2  0.48]\n",
      " [0.36 0.18 0.12 0.6  0.44]\n",
      " [0.85 0.49 0.24 0.45 0.73]\n",
      " [0.6  0.32 0.18 0.38 0.49]\n",
      " [0.46 0.26 0.14 0.58 0.55]]\n",
      "idx 0. masked\n",
      " [[0.7  -inf -inf -inf -inf]\n",
      " [0.36 0.18 -inf -inf -inf]\n",
      " [0.85 0.49 0.24 -inf -inf]\n",
      " [0.6  0.32 0.18 0.38 -inf]\n",
      " [0.46 0.26 0.14 0.58 0.55]]\n",
      "idx 0. softmax\n",
      " [[1.         0.         0.         0.         0.        ]\n",
      " [0.54487889 0.45512111 0.         0.         0.        ]\n",
      " [0.44622395 0.31131988 0.24245617 0.         0.        ]\n",
      " [0.31100819 0.23505494 0.20434695 0.24958992 0.        ]\n",
      " [0.2097953  0.17176587 0.15234266 0.23654354 0.22955263]]\n",
      "idx 0. probs check\n",
      " [1. 1. 1. 1. 1.]\n",
      "Full output shape (2, 5, 3)\n"
     ]
    }
   ],
   "source": [
    "# Causal self-attention\n",
    "mask = np.where(np.triu(np.ones((C, C)), k=1) == 1, -np.inf, 0) # Fill upper triangle with -inf\n",
    "print(\"idx 0. mask\\n\", mask)\n",
    "print(\"idx 0. logit\\n\", logits_regd[0].round(2))\n",
    "print(\"idx 0. masked\\n\", logits_regd[0].round(2) + mask)\n",
    "print(\"idx 0. softmax\\n\", softmax(logits_regd[0].round(2) + mask))\n",
    "print(\"idx 0. probs check\\n\", softmax(logits_regd[0].round(2) + mask).sum(-1))\n",
    "\n",
    "causal_self_attention = softmax(logits_regd + mask) @ v\n",
    "print(\"Full output shape\", causal_self_attention.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
