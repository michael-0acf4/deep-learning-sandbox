{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# utils\n",
    "def debug_att(source, mat, who, to):\n",
    "    print(f\"'{who}' attends '{to}': {(mat[source.index(who), source.index(to)] * 100).round()}%\")\n",
    "\n",
    "def debug_x_att(source, target, mat, who, to):\n",
    "    print(f\"'{who}' attends '{to}': {(mat[source.index(who), target.index(to)] * 100).round()}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppose we are in the middle of the training dataset (e.g. a text)\n",
    "# and we are at some batch B (the attention mechanism allows us to process B batches concurrently)\n",
    "\n",
    "np.random.seed(1234)\n",
    "\n",
    "tokens_eng = [\"nah\", \"I\", \"d\", \"win\"]\n",
    "tokens_jp = [\"いいえ\", \"勝つ\", \"さ\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$attention := softmax(Query(s_q) . Key(s_k)^T) . Value(s_v)$$\n",
    "\n",
    "Or with proper scaling, assuming $Q$ and $K$ both came from a $N(\\mu = 0, \\sigma^2 = 1)$, we get..\n",
    "\n",
    "$$attention := softmax(\\frac{Query(s_q) . Key(s_k)^T} {\\sqrt{EmbeddingDim}}) . Value(s_v)$$\n",
    "\n",
    "Note: $Var(Q. K) = \\sum_i {Q_{i} K_{i}} =  \\sum_i 1 . 1 = EmbeddingDim$ hence the denominator to keep everything stable by scaling back the variance to be 1 due to softmax behaviors on large values.\n",
    "\n",
    "$s_q, s_k, s_v$ are respectively the input \"sources\" for the Query, Key and Value matrices.\n",
    "\n",
    "Commonly, $s_k = s_v= t$, $s_q$ gets refered as query source $s$ whereas $t$ is the target source.\n",
    "\n",
    "Why does attention take that form? It's **ad hoc** assumptions on top of **well founded intuition**. \\\n",
    "**As far as I know** there are no real findings yet on why it works so well on arbitrary distributions.\n",
    "\n",
    "On the **well founded intuition** part, the entire thing is not that crazy at all, key is to define mathematically what \"similarity\" means.. \\\n",
    "For vectors, this can be defined in many ways: cosine similarity, dot product, absolute distance, ..etc.\n",
    "For attention, we commonly use dot product.\n",
    "\n",
    "$Q$ is a matrix embedding of the source, each rows is a sub-embedding of some concept such that when we compute the similarity of the \\\n",
    "i-th row with the key $Q_i . K^T$, we get a number representing the affinities between the two. Then we do that for all rows... \\\n",
    "All these ops are compactified in matrix form.\n",
    "\n",
    "When the source of the query is the same as the key it is a decoder architecture, when they are different, it is an encoder/decoder architecture \\\n",
    "as we compute the similarity between a source and a target source, which is literally inducing an embedding of a translation from source to target \\\n",
    "in some sense, and is also the main topic of the [Attention is All You Need](https://arxiv.org/abs/1706.03762) paper.\n",
    "\n",
    "Why go all the trouble with intermediary embeddings $Q$, $K$, and $V$? \\\n",
    "The answer is simple, we want to generalize by holding the sources as private and instead we delegate representation with intermediary vector spaces. \\\n",
    "This whole scheme can also be viewed as a lossy compression of the sources, Q compresses the main source, K the target source.\n",
    "\n",
    "The $softmax(Query . Key)$ component especially can be viewed as a directed graph.\n",
    "* Each token is a node\n",
    "* The weight of each connection is expected to quantify how much a source token \"cares\" about another, \\\n",
    "  this value gets coupled with the source and target.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'I' attends 'win': 20.0%\n",
      "'win' attends 'I': 50.0%\n",
      "'d' attends 'nah': 30.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Self-attention: the example attends to itself i.e. s_q, s_k, s_z all come from the same source (the example)\n",
    "softmax_qk = np.array([ \n",
    "    # nah  I   d    win\n",
    "    [0.1, 0.2, 0.3, 0.4], # nah\n",
    "    [0.1, 0.4, 0.3, 0.2], # I\n",
    "    [0.3, 0.2, 0.2, 0.3], # d\n",
    "    [0.3, 0.5, 0.1, 0.1], # win\n",
    "])\n",
    "\n",
    "debug_att(tokens_eng, softmax_qk, \"I\", \"win\")\n",
    "debug_att(tokens_eng, softmax_qk, \"win\", \"I\")\n",
    "debug_att(tokens_eng, softmax_qk, \"d\", \"nah\")\n",
    "softmax_qk.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'I' attends 'win': 0.0%\n",
      "'win' attends 'I': 50.0%\n",
      "'d' attends 'nah': 80.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1., 1., 1.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Causal self-attention: self-attention with a mask\n",
    "\n",
    "# Goal: each token at position t cannot attend tokens after >= t + 1. (which we care in Generative models like GPTs or word2vec embeddings like BERT)\n",
    "# Practically, it is just self-attention applied with a mask that hides the upper diag\n",
    "# Simply done by replacing the components above the diagonal with -Infinity, softmax will 0 on these\n",
    "# and fix the probabilities on the finite components.\n",
    "\n",
    "softmax_qk = np.array([ \n",
    "    # nah  I   d    win\n",
    "    [1.0, 0.0, 0.0, 0.0], # nah\n",
    "    [0.2, 0.8, 0.0, 0.0], # I\n",
    "    [0.8, 0.1, 0.1, 0.0], # d\n",
    "    [0.3, 0.5, 0.1, 0.1], # win\n",
    "])\n",
    "\n",
    "debug_att(tokens_eng, softmax_qk, \"I\", \"win\") # cannot see into the future\n",
    "debug_att(tokens_eng, softmax_qk, \"win\", \"I\")\n",
    "debug_att(tokens_eng, softmax_qk, \"d\", \"nah\")\n",
    "softmax_qk.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'I' attends 'いいえ': 30.0%\n",
      "'win' attends '勝つ': 80.0%\n",
      "'I' attends '勝つ': 60.0%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1.  , 1.  , 0.99, 1.  ])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cross attention: Query comes from the main source, Key from a target source\n",
    "softmax_qk = np.array([ \n",
    "    # いいえ  勝つ    さ\n",
    "    [ 0.9, 0.05, 0.05],  # nah\n",
    "    [ 0.3,  0.6,  0.1],  # I\n",
    "    [0.33, 0.33, 0.33],  # d\n",
    "    [0.15,  0.8, 0.05],  # win\n",
    "])\n",
    "\n",
    "debug_x_att(tokens_eng, tokens_jp, softmax_qk, \"I\", \"いいえ\")\n",
    "debug_x_att(tokens_eng, tokens_jp, softmax_qk, \"win\", \"勝つ\")\n",
    "debug_x_att(tokens_eng, tokens_jp, softmax_qk, \"I\", \"勝つ\")\n",
    "softmax_qk.sum(axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flash attention\n",
    "\n",
    "# Nothing much to be said here, it's just yet another trick for reshaping the math ops on the GPU\n",
    "# Original: https://arxiv.org/abs/2205.14135\n",
    "# Example improv proposal: https://arxiv.org/abs/2307.08691"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
