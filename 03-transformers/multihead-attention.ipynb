{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why?\n",
    "\n",
    "One key component with the transformer architecture (making up about 60% of it) is the Multiheaded attention.\\\n",
    "There is nothing magical about it either, it is just attention but chunked into N parts on the embedding dim.\n",
    "\n",
    "Practically, $Query$, $Key$ and $Value$ are all of shape $(C, E)$ where $C$ is the input dim and $E$ is the embedding dim . \\\n",
    "Meaning... $softmax(\\frac {Query . Key^T} {\\sqrt {E}} + Mask) . Value$ is of shape $(C, E)$\n",
    "\n",
    "In the multiheaded scenario... we chunk the $Query$, $Key$ and $Value$ by the number of heads we want (that would be an hyperparameter for the model).\n",
    "\n",
    "i.e. for $N$ heads we have $N$ small $Query$, $Key$ and $Value$, each of shape $(C, E / N)$, for simplicity $N$ should divide $E$.\n",
    "\n",
    "But how do we merge it back? \\\n",
    "Concatenation. \\\n",
    "Like.. literally... This is yet again non-rigourous ad hoc solution by DL folks. But intuitively, we are still on the wrong space! as $softmax$ do not have such fancy convenient properties.\n",
    "\n",
    "The other trick is to have yet another intermediary space that would project the ad hoc concatenation into the actual/original expected embedding space.\n",
    "\n",
    "$$(Head_1 \\oplus Head_2 \\oplus ... \\oplus Head_N) \\xrightarrow{} raw concat \\xrightarrow{proj} original$$\n",
    "$$(C, E / N) \\oplus (C, E / N) \\oplus ... \\oplus (C, E / N) \\xrightarrow{} (C, E) \\xrightarrow{proj} (C, E) $$\n",
    "\n",
    "$proj$ can be as simple as (yet) another linear transformation that can be learned through training. \n",
    "\n",
    "i.e. $$attention = proj(Head_1 \\oplus Head_2 \\oplus ... \\oplus Head_N) $$\n",
    "with $$Head_i = softmax(\\frac {Split(Query) . Split(Key)^T} {\\sqrt {E / N}} + Mask) . Split(Value)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    x_exp = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
    "    return x_exp / np.sum(x_exp, axis=-1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "B, C, E = 3, 11, 8 # batch, context length, embedding dim\n",
    "q = np.random.rand(B, C, E)\n",
    "k = np.random.rand(C, E)\n",
    "v = np.random.rand(C, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 11, 8)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# classic (no mask for simplicity)\n",
    "classic_attention = softmax((q @ k.T) / np.sqrt(E)) @ v\n",
    "classic_attention.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "concat (3, 11, 8)\n",
      "proj(concat) -> attention (3, 11, 8)\n"
     ]
    }
   ],
   "source": [
    "# multiheaded\n",
    "N = 4\n",
    "assert E % N == 0\n",
    "\n",
    "proj = np.random.rand(E, E) # learned at the same time\n",
    "\n",
    "split_dim = E // N\n",
    "heads = []\n",
    "\n",
    "for i in range(N):\n",
    "    q_head = q[:, :, i * split_dim: (i + 1) * split_dim]    # (B, C, E / N)\n",
    "    k_head = k[   :, i * split_dim: (i + 1) * split_dim]    # (C, E / N)\n",
    "    v_head = v[   :, i * split_dim: (i + 1) * split_dim]    # (C, E / N)\n",
    "    # print(q_head.shape, k_head.T.shape, v_head.shape)\n",
    "\n",
    "    scores = q_head @ k_head.T / np.sqrt(split_dim)         # (B, C, C)\n",
    "    attention_weights = softmax(scores)                     # (B, C, C)\n",
    "\n",
    "    # weighted sum of values\n",
    "    local_head = attention_weights @ v_head                 # (B, C, E / N)\n",
    "    heads.append(local_head)\n",
    "\n",
    "raw_multi_head_output = np.concatenate(heads, axis=-1)      # (B, C, E)\n",
    "print(\"concat\", raw_multi_head_output.shape)\n",
    "\n",
    "attention = raw_multi_head_output @ proj                    # (B, C, E) . (E, E) -> (B, C, E)\n",
    "print(\"proj(concat) -> attention\", raw_multi_head_output.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
